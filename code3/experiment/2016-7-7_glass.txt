#日期：2016年7月7日

实验记录：

1，模型： 无边界检测，bagging训练样本，构造多个决策树，之后得到一个准线性核矩阵，生成一个QLSVM分类器。

2，对比分类器： RBFSVM , LinearSVM

3，数据集：glass

4，实验总结：


5，实验结果： CV=3(shuffle=True), glass=(214L, 9L),
			  pos(class=6)=9,neg(others)=205
	1）RBF精度在cv测试中，精度最低，说明过拟合的很厉害
		Mean validation precision_score: 0.722 (std: 0.208)
		Mean validation recall_score: 0.778 (std: 0.157)
		Mean validation f1_score: 0.746 (std: 0.184)

		Mean validation precision_score: 0.778 (std: 0.157)
		Mean validation recall_score: 0.778 (std: 0.157)
		Mean validation f1_score: 0.778 (std: 0.157)

		Mean validation precision_score: 0.694 (std: 0.039)
		Mean validation recall_score: 0.778 (std: 0.157)
		Mean validation f1_score: 0.730 (std: 0.090)

	2）Linear精度在cv测试中表现出了比RBF更好的泛化能力。
		Mean validation precision_score: 0.889 (std: 0.157)
		Mean validation recall_score: 0.778 (std: 0.157)
		Mean validation f1_score: 0.822 (std: 0.137)

		Mean validation precision_score: 0.806 (std: 0.142)
		Mean validation recall_score: 0.889 (std: 0.157)
		Mean validation f1_score: 0.841 (std: 0.137)


	3）QLinear精度在cv测试中，最好，QL的好处是对C不敏感.
		myFore = my_RF_QLSVM.RF_QLSVM_clf(n_trees=30, 
	                    leafType='LogicReg', errType='lseErr_regul',
	                    max_depth=None, min_samples_split=3,
	                    max_features='log2',bootstrap_data=True)
	  	k=1, 叶子节点聚类。
	  	#X1_delta = X[y==1] - X0_mean # (m,n)
        #X0_delta = X[y==0] - X1_mean
		 {1: [[0.88888888888888884, 0.15713484026367724],
		  [0.77777777777777768, 0.15713484026367724],
		  [0.8222222222222223, 0.13698697784375505]],
		 2: [[0.75, 0.20412414523193151],
		  [0.77777777777777779, 0.31426968052735443],
		  [0.75238095238095237, 0.25590531128656002]],
		 3: [[0.69444444444444453, 0.27498597046143514],
		  [0.66666666666666663, 0.27216552697590868],
		  [0.60052910052910058, 0.18286528739455507]],
		 4: [[0.83333333333333337, 0.23570226039551584],
		  [0.66666666666666663, 0.27216552697590868],
		  [0.73333333333333339, 0.24944382578492941]],
		 5: [[0.58333333333333337, 0.42491829279939874],
		  [0.55555555555555547, 0.41573970964154905],
		  [0.55238095238095231, 0.39128835398426925]],
		 6: [[0.91666666666666663, 0.11785113019775792],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.88571428571428579, 0.084112008250741388]],
		 7: [[1.0, 0.0],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.93333333333333324, 0.094280904158206322]]}

		  {1: [[0.88888888888888884, 0.15713484026367724],
		  [0.77777777777777768, 0.15713484026367724],
		  [0.8222222222222223, 0.13698697784375505]],
		 2: [[0.6333333333333333, 0.26246692913372704],
		  [0.66666666666666663, 0.27216552697590868],
		  [0.6333333333333333, 0.26246692913372704]],
		 3: [[0.72222222222222221, 0.20786985482077452],
		  [0.77777777777777768, 0.15713484026367724],
		  [0.74603174603174605, 0.18374344290143213]],
		 4: [[0.88888888888888884, 0.15713484026367724],
		  [0.77777777777777768, 0.15713484026367724],
		  [0.8222222222222223, 0.13698697784375505]],
		 5: [[0.78333333333333333, 0.16499158227686112],
		  [1.0, 0.0],
		  [0.86904761904761907, 0.10240863413145988]],
		 6: [[0.83333333333333337, 0.11785113019775792],
		  [1.0, 0.0],
		  [0.90476190476190477, 0.067343502970147406]],
		 7: [[0.75, 0.20412414523193151],
		  [0.66666666666666663, 0.27216552697590868],
		  [0.68571428571428583, 0.20337291908631067]],


		myFore = my_RF_QLSVM.RF_QLSVM_clf(n_trees=30, 
	                    leafType='LogicReg', errType='lseErr_regul',
	                    max_depth=None, min_samples_split=5,
	                    max_features='log2',bootstrap_data=True)
	  	k=1, 叶子节点聚类。
	  	X_delta = X - np.mean(X, axis=0)
	  	{1: [[0.88888888888888884, 0.15713484026367724],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.88888888888888884, 0.15713484026367724]],
		 2: [[0.69444444444444453, 0.27498597046143514],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.7671957671957671, 0.23555374909578905]],
		 3: [[0.6333333333333333, 0.26246692913372704],
		  [0.55555555555555547, 0.15713484026367722],
		  [0.56666666666666676, 0.16996731711975951]],
		 4: [[0.66666666666666663, 0.11785113019775792],
		  [0.77777777777777768, 0.31426968052735449],
		  [0.70476190476190481, 0.2154992095044716]],
		 5: [[0.83333333333333337, 0.23570226039551584],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.85714285714285721, 0.20203050891044211]],
		 6: [[0.8666666666666667, 0.18856180831641267],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.84999999999999998, 0.10801234497346436]],
		 7: [[0.91666666666666663, 0.11785113019775792],
		  [1.0, 0.0],
		  [0.95238095238095244, 0.067343502970147406]]}

		  {1: [[0.80555555555555547, 0.14163943093313291],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.84126984126984128, 0.13654484550861315]],
		 2: [[0.61111111111111105, 0.28327886186626583],
		  [0.66666666666666663, 0.27216552697590868],
		  [0.61481481481481481, 0.27297075218797451]],
		 3: [[0.58888888888888891, 0.068493488921877496],
		  [0.66666666666666663, 0.27216552697590868],
		  [0.60555555555555551, 0.14927809825049332]],
		 4: [[0.72222222222222221, 0.20786985482077452],
		  [0.77777777777777768, 0.15713484026367724],
		  [0.74603174603174605, 0.18374344290143213]],
		 5: [[0.72222222222222221, 0.20786985482077452],
		  [0.66666666666666663, 0.27216552697590868],
		  [0.68888888888888877, 0.24545246704860579]],
		 6: [[0.72619047619047616, 0.23389146076652975],
		  [1.0, 0.0],
		  [0.81904761904761914, 0.16550616378078825]],
		 7: [[0.6333333333333333, 0.26246692913372704],
		  [0.55555555555555547, 0.15713484026367722],
		  [0.56666666666666676, 0.16996731711975951]]}

		  {1: [[0.80555555555555547, 0.14163943093313291],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.84126984126984128, 0.13654484550861315]],
		 2: [[0.71666666666666667, 0.24608038433722332],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.7857142857142857, 0.21028002062685347]],
		 3: [[0.88888888888888884, 0.15713484026367724],
		  [0.77777777777777768, 0.15713484026367724],
		  [0.8222222222222223, 0.13698697784375505]],
		 4: [[0.71666666666666667, 0.24608038433722332],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.7857142857142857, 0.21028002062685347]],
		 5: [[0.83333333333333337, 0.11785113019775792],
		  [1.0, 0.0],
		  [0.90476190476190477, 0.067343502970147406]],
		 6: [[0.91666666666666663, 0.11785113019775792],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.88571428571428579, 0.084112008250741388]],
		 7: [[0.91666666666666663, 0.11785113019775792],
		  [0.88888888888888884, 0.15713484026367724],
		  [0.88571428571428568, 0.084112008250741388]]}